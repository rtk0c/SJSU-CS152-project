\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
  T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}






\begin{document}

\title{CS 152 Project Proposal}

\author{
  \IEEEauthorblockN{1\textsuperscript{st} Tianyi Guan}
  \IEEEauthorblockA{\textit{San Jose State University}\\
  California, United States \\
	tianyi.guan@sjsu.edu}
  \and
  \IEEEauthorblockN{2\textsuperscript{nd} Emily Williams}
  \IEEEauthorblockA{\textit{San Jose State University}\\
  California, United States \\
	emily.williams01@sjsu.edu}
}

\maketitle


\begin{abstract}
This document is a proposal for the CS 152 course project.
We propose a code golfing problem solver system based on using LLM to provide natural language understanding of the problem, followed by using a traditional parsing based code minifier to ``golf'' the solution.
\end{abstract}


\section{Overview}
For the CS 152 coures project, we want to design a code golfing solver system, consisting of a LLM solver component and a traditional minifier component.

\section{Solver Component}
In solver component, a LLM is prompted to read the input problem description written in English, and output a solution written in Python. We constrain the input problem space to those found in the ARC-AGI dataset \cite{b1} \cite{b2}.
Each problem description consists of several example pairs of desired input/output, each being a 2d grid of different integers. The goal of the problem is to extrapolate a pattern from the examples, and write a Python function doing the transformation, using as little code as possible.
The size of a solution is measured by the number of bytes of the resulting program text.
We ignore the size requirement (``golfing'') from the LLM component, instead leaving it to the minifier component.

We will use multi-shot prompting to let the LLM deduce the pattern from the example input/outputs in English, where LLMs excel in processing.
Then, we will use LLM (potentially different model) let it generate the solution Python code from the problem description.
The LLM will inevitably give incorrect solutions. We need to categorize for common errors in the solution generated by LLM, such as symmetry color mismatch. Each error type should be assigned a unique error code, and have a corresponding template containing actionable feedback for reprompting.

\section{Minifier Component}
The minifier component will use tradition parsing- and pattern-matching-based techniques to reduce the solution.
Our ideal solution is a hybrid of AST structural reduction, and regex cleanser to remove extraneous whitespace.
We will use CPython's AST module to obtain a parse tree of the pre-goled solution. Then, we will recursively traverse the parse tree, replacing known patterns with equivalent shorter forms.
For example, given some array \verb|arr|, a standalone statement with \verb|arr.reverse()| to reverse it can be replaced with \verb|arr[::-1]| at its usages.
A bigger example: a conditional move expression \verb|foo if cond else bar| can be replaced with \verb|[foo,bar][cond]| if foo and bar can be both evaluated (no side effect).
There may be some edge cases the AST solution glazes over when focusing a specific part instead of the entire context of the problem, so it could lose nuance. 

A simpler alernative to AST structural reduction is a pure scanning/regex based mechanism that operates strictly over the program text, without getting a parse tree. This method in theory provides lower implementation complexity, at the cost of being less effective.
It is not yet clear that which approach will make the most sense for this project. This is a question which we want to continuously investigate.

\section{Summary}
The end product should be a CLI program that takes a JSON-formatted ARC-AGI problem, (1) generates the pre-golfed solution through LLM, (2) validate against the testsuite, (3) repeate until no progress can be made or completely correct, (4) minify, and finally (5) output solution.
Optionally, intermediates such as the result of each shot in LLM prompting, or middle stages of the minifier can be dumped, to provide debugging visibility into the inner systems.

\begin{thebibliography}{00}
\bibitem{b1} Fran√ßois Chollet. ``On the Measure of Intelligence,'' in CoRR, vol. abs/1911.01547, 2019.
\bibitem{b2} https://arcprize.org
\bibitem{b3} Dflook, ``Dflook/python-minifier: Transform python source code into its most compact representation,'' GitHub, https://github.com/dflook/python-minifier (accessed Sep. 13, 2025). 
\bibitem{b4} ``terser/terser: JavaScript parser, mangler and compressor toolkit for ES6+'' GitHub, https://github.com/terser/terser (accessed Sep. 13, 2025). 
\end{thebibliography}

\end{document}
